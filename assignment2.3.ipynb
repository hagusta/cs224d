{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils import calculate_perplexity, get_ptb_dataset, Vocab\n",
    "from utils import ptb_iterator, sample\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.seq2seq import sequence_loss\n",
    "from model import LanguageModel\n",
    "\n",
    "from q2_initialization import xavier_weight_init\n",
    "\n",
    "class Config(object):\n",
    "  \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "  The config class is used to store various hyperparameters and dataset\n",
    "  information parameters. Model objects are passed a Config() object at\n",
    "  instantiation.\n",
    "  \"\"\"\n",
    "  batch_size = 64\n",
    "  embed_size = 50\n",
    "  hidden_size = 100\n",
    "  num_steps = 10\n",
    "  max_epochs = 2\n",
    "  #max_epochs = 10\n",
    "  early_stopping = 2\n",
    "  dropout = 0.9\n",
    "  lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929589.0 total words with 10000 uniques\n",
      "929589.0 total words with 10000 uniques\n",
      "Epoch 0\n",
      "Training perplexity: 560.9331665039062\n",
      "Validation perplexity: 403.7873229980469\n",
      "Total time: 48.57476854324341\n",
      "Epoch 1\n",
      "Training perplexity: 330.63525390625\n",
      "Validation perplexity: 300.0437927246094\n",
      "Total time: 49.14159321784973\n",
      "=-==-==-==-==-=\n",
      "Test perplexity: 283.8559875488281\n",
      "=-==-==-==-==-=\n",
      "in palo alto jacobs statistical counted koch cracks invested year sitting outlets living democrat undoubtedly violate pour revise lid keenan town acceleration mci lying economist round va. marine maurice sheets cable-tv greenhouse letter named acquisition kemp musicians plains blast liable improperly agnelli founder pressed trustee served illustrates prepared vast flavor strokes limited berry commodore memory intent gift pose benjamin lives crusade cross-border bloody severance vicar bryant versions ratners singer hot natural-gas harper vehicles fort clutter advocates a. prince technicians c$ \\* cup patience single-a confirmed served exercise bailout lufkin imposes legislative resorts purchases yetnikoff tanker counseling convey matches undo serve golden edison foster\n",
      "> \n"
     ]
    }
   ],
   "source": [
    "class RNNLM_Model(LanguageModel):\n",
    "\n",
    "  def load_data(self, debug=False):\n",
    "    \"\"\"Loads starter word-vectors and train/dev/test data.\"\"\"\n",
    "    self.vocab = Vocab()\n",
    "    self.vocab.construct(get_ptb_dataset('train'))\n",
    "    self.encoded_train = np.array(\n",
    "        [self.vocab.encode(word) for word in get_ptb_dataset('train')],\n",
    "        dtype=np.int32)\n",
    "    self.encoded_valid = np.array(\n",
    "        [self.vocab.encode(word) for word in get_ptb_dataset('valid')],\n",
    "        dtype=np.int32)\n",
    "    self.encoded_test = np.array(\n",
    "        [self.vocab.encode(word) for word in get_ptb_dataset('test')],\n",
    "        dtype=np.int32)\n",
    "    if debug:\n",
    "      num_debug = 1024\n",
    "      self.encoded_train = self.encoded_train[:num_debug]\n",
    "      self.encoded_valid = self.encoded_valid[:num_debug]\n",
    "      self.encoded_test = self.encoded_test[:num_debug]\n",
    "        \n",
    "\n",
    "  def add_placeholders(self):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors\n",
    "\n",
    "    These placeholders are used as inputs by the rest of the model building\n",
    "    code and will be fed data during training.  Note that when \"None\" is in a\n",
    "    placeholder's shape, it's flexible\n",
    "\n",
    "    Adds following nodes to the computational graph.\n",
    "    (When None is in a placeholder's shape, it's flexible)\n",
    "\n",
    "    input_placeholder: Input placeholder tensor of shape\n",
    "                       (None, num_steps), type tf.int32\n",
    "    labels_placeholder: Labels placeholder tensor of shape\n",
    "                        (None, num_steps), type tf.float32\n",
    "    dropout_placeholder: Dropout value placeholder (scalar),\n",
    "                         type tf.float32\n",
    "\n",
    "    Add these placeholders to self as the instance variables\n",
    "  \n",
    "      self.input_placeholder\n",
    "      self.labels_placeholder\n",
    "      self.dropout_placeholder\n",
    "\n",
    "    (Don't change the variable names)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    config=self.config\n",
    "    with tf.name_scope(\"input\"):\n",
    "        self.input_placeholder=tf.placeholder(tf.int32,(None,config.num_steps),name=\"input_placeholder\")\n",
    "    with tf.name_scope(\"labels\"):\n",
    "        self.labels_placeholder=tf.placeholder(tf.int64,(None,config.num_steps),name=\"labels_placeholder\")\n",
    "    with tf.name_scope(\"drop_out\"):\n",
    "        self.dropout_placeholder=tf.placeholder(tf.float32,name=\"dropout_placeholder\")\n",
    "    ### END YOUR CODE\n",
    "  \n",
    "  def add_embedding(self):\n",
    "    \"\"\"Add embedding layer.\n",
    "\n",
    "    Hint: This layer should use the input_placeholder to index into the\n",
    "          embedding.\n",
    "    Hint: You might find tf.nn.embedding_lookup useful.\n",
    "    Hint: You might find tf.split, tf.squeeze useful in constructing tensor inputs\n",
    "    Hint: Check the last slide from the TensorFlow lecture.\n",
    "    Hint: Here are the dimensions of the variables you will need to create:\n",
    "\n",
    "      L: (len(self.vocab), embed_size)\n",
    "\n",
    "    Returns:\n",
    "      inputs: List of length num_steps, each of whose elements should be\n",
    "              a tensor of shape (batch_size, embed_size).\n",
    "    \"\"\"\n",
    "    # The embedding lookup is currently only implemented for the CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "      ### YOUR CODE HERE\n",
    "        embedding=tf.Variable(tf.random_uniform(\n",
    "                (len(self.vocab),self.config.embed_size),minval=0,maxval=1,dtype=tf.float32),name=\"embedding\" )\n",
    "        embed=tf.nn.embedding_lookup(embedding, self.input_placeholder)\n",
    "      #print(embed.get_shape())\n",
    "        inputs= [tf.squeeze(s,squeeze_dims=(1,)) for s in tf.split(1,self.config.num_steps,embed)]\n",
    "      ### END YOUR CODE\n",
    "    return inputs\n",
    "\n",
    "  def add_model(self, inputs):\n",
    "    \"\"\"Creates the RNN LM model.\n",
    "\n",
    "    In the space provided below, you need to implement the equations for the\n",
    "    RNN LM model. Note that you may NOT use built in rnn_cell functions from\n",
    "    tensorflow.\n",
    "\n",
    "    Hint: Use a zeros tensor of shape (batch_size, hidden_size) as\n",
    "          initial state for the RNN. Add this to self as instance variable\n",
    "\n",
    "          self.initial_state\n",
    "  \n",
    "          (Don't change variable name)\n",
    "    Hint: Add the last RNN output to self as instance variable\n",
    "\n",
    "          self.final_state\n",
    "\n",
    "          (Don't change variable name)\n",
    "    Hint: Make sure to apply dropout to the inputs and the outputs.\n",
    "    Hint: Use a variable scope (e.g. \"RNN\") to define RNN variables.\n",
    "    Hint: Perform an explicit for-loop over inputs. You can use\n",
    "          scope.reuse_variables() to ensure that the weights used at each\n",
    "          iteration (each time-step) are the same. (Make sure you don't call\n",
    "          this for iteration 0 though or nothing will be initialized!)\n",
    "    Hint: Here are the dimensions of the various variables you will need to\n",
    "          create:\n",
    "      \n",
    "          H: (hidden_size, hidden_size) \n",
    "          I: (embed_size, hidden_size)\n",
    "          b_1: (hidden_size,)\n",
    "\n",
    "    Args:\n",
    "      inputs: List of length num_steps, each of whose elements should be\n",
    "              a tensor of shape (batch_size, embed_size).\n",
    "    Returns:\n",
    "      outputs: List of length num_steps, each of whose elements should be\n",
    "               a tensor of shape (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    with tf.name_scope(\"initial_state\"):\n",
    "        config=self.config\n",
    "        self.initial_state=tf.zeros((config.batch_size,config.hidden_size),name=\"initial_state\")\n",
    "    #self.initial_state = tf.zeros((self.config.batch_size, self.config.hidden_size))\n",
    "        state=self.initial_state\n",
    "                        \n",
    "    xavier_x=xavier_weight_init()\n",
    "    with tf.name_scope(\"weight_H\"):\n",
    "        self.H=tf.Variable(xavier_x((config.hidden_size, config.hidden_size)),name=\"H\")\n",
    "\n",
    "    with tf.name_scope(\"weight_I\"):\n",
    "        self.I=tf.Variable(xavier_x((config.embed_size,config.hidden_size)),name=\"I\")\n",
    "\n",
    "    with tf.name_scope(\"bias_b1\"):\n",
    "        self.b_1=tf.Variable(xavier_x((config.hidden_size,)),name=\"b_1\")\n",
    "\n",
    "        \n",
    "    rnn_outputs=[]    \n",
    "    for i in range(config.num_steps):\n",
    "        state=tf.nn.sigmoid(tf.matmul(state,self.H) + \n",
    "                                   tf.matmul(tf.nn.dropout(inputs[i],config.dropout),self.I)+self.b_1) \n",
    "        with tf.name_scope(\"final_state\"):\n",
    "            self.final_state=state\n",
    "        rnn_outputs.append(state)\n",
    "                                           \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return rnn_outputs\n",
    "                                   \n",
    "  def add_projection(self, rnn_outputs):\n",
    "    \"\"\"Adds a projection layer.\n",
    "\n",
    "    The projection layer transforms the hidden representation to a distribution\n",
    "    over the vocabulary.\n",
    "\n",
    "    Hint: Here are the dimensions of the variables you will need to\n",
    "          create \n",
    "          \n",
    "          U:   (hidden_size, len(vocab))\n",
    "          b_2: (len(vocab),)\n",
    "\n",
    "    Args:\n",
    "      rnn_outputs: List of length num_steps, each of whose elements should be\n",
    "                   a tensor of shape (batch_size, embed_size).\n",
    "    Returns:\n",
    "      outputs: List of length num_steps, each a tensor of shape\n",
    "               (batch_size, len(vocab)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    config=self.config\n",
    "    xavier_w=xavier_weight_init()\n",
    "    with tf.name_scope(\"weight_U\"):\n",
    "        self.U=tf.Variable(xavier_w((config.hidden_size,len(self.vocab))),name=\"U\")\n",
    "        \n",
    "    with tf.name_scope(\"bias_b2\"):\n",
    "        self.b_2=tf.Variable(xavier_w((len(self.vocab),)),name=\"b_2\")\n",
    "        \n",
    "    outputs=[]\n",
    "    for i in range(config.num_steps):\n",
    "            #print(\"rnn_outputs[i]:\",rnn_outputs[i].get_shape())\n",
    "        outputs.append(tf.matmul(tf.nn.dropout(rnn_outputs[i],self.config.dropout),self.U)+self.b_2)\n",
    "    ### END YOUR CODE\n",
    "        \n",
    "        \n",
    "    return outputs\n",
    "\n",
    "  def add_loss_op(self, output):\n",
    "    \"\"\"Adds loss ops to the computational graph.\n",
    "\n",
    "    Hint: Use tensorflow.python.ops.seq2seq.sequence_loss to implement sequence loss. \n",
    "\n",
    "    Args:\n",
    "      output: A tensor of shape (None, self.vocab)\n",
    "    Returns:\n",
    "      loss: A 0-d tensor (scalar)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #print(\"output:\",output.get_shape())\n",
    "    #print(\"self.config.num_steps*self.config.batch_size:\",self.config.num_steps*self.config.batch_size)\n",
    "    #print(\"self.config.num_steps:\",self.config.num_steps)\n",
    "    #print(\"self.config.batch_size:\",self.config.batch_size)\n",
    "    loss=sequence_loss([output],\n",
    "                       [tf.reshape(self.labels_placeholder,[self.config.num_steps*self.config.batch_size,-1])],\n",
    "                       [tf.constant(1.0)])\n",
    "    ### END YOUR CODE\n",
    "    #tf.summary.scalar(\"loss\",loss)\n",
    "    return loss\n",
    "\n",
    "  def add_training_op(self, loss):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train. See \n",
    "\n",
    "    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "    for more information.\n",
    "\n",
    "    Hint: Use tf.train.AdamOptimizer for this model.\n",
    "          Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "    Args:\n",
    "      loss: Loss tensor, from cross_entropy_loss.\n",
    "    Returns:\n",
    "      train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #print(\"lr:\",self.config.lr)\n",
    "    optimizer=tf.train.AdamOptimizer(self.config.lr,name=\"TrainAdamOpt\")\n",
    "    train_op=optimizer.minimize(loss)\n",
    "    ### END YOUR CODE\n",
    "    return train_op\n",
    "  \n",
    "  def __init__(self, config):\n",
    "    self.config = config\n",
    "    self.load_data(debug=False)\n",
    "    #self.load_data(debug=True)\n",
    "    with tf.name_scope(\"feed\"):\n",
    "        self.add_placeholders()\n",
    "    with tf.name_scope(\"embed\"):\n",
    "        self.inputs = self.add_embedding()\n",
    "    with tf.name_scope(\"hidden_layer\"):\n",
    "        self.rnn_outputs = self.add_model(self.inputs)\n",
    "    with tf.name_scope(\"projection_layer\"):\n",
    "        self.outputs = self.add_projection(self.rnn_outputs)\n",
    "  \n",
    "    # We want to check how well we correctly predict the next word\n",
    "    # We cast o to float64 as there are numerical issues at hand\n",
    "    # (i.e. sum(output of softmax) = 1.00000298179 and not 1)\n",
    "    with tf.name_scope(\"prediction\"):\n",
    "        self.predictions = [tf.nn.softmax(tf.cast(o, 'float64')) for o in self.outputs]\n",
    "    # Reshape the output into len(vocab) sized chunks - the -1 says as many as\n",
    "    # needed to evenly divide\n",
    "    output = tf.reshape(tf.concat(1, self.outputs), [-1, len(self.vocab)])\n",
    "    #print(\"calc loss 1\")\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        self.calculate_loss = self.add_loss_op(output)\n",
    "        tf.summary.scalar(\"loss\",self.calculate_loss)\n",
    "    #print(\"calc loss 2\")\n",
    "    with tf.name_scope(\"train\"):\n",
    "        self.train_step = self.add_training_op(self.calculate_loss)\n",
    "    #print(\"calc loss 3\")\n",
    "    self.merged = tf.summary.merge_all()\n",
    "    self.writer = tf.summary.FileWriter(\"/tmp/logs/\")\n",
    "\n",
    "  def run_epoch(self, session, data, train_op=None, verbose=10):\n",
    "    config = self.config\n",
    "    dp = config.dropout\n",
    "    if not train_op:\n",
    "      train_op = tf.no_op()\n",
    "      dp = 1\n",
    "    total_steps = sum(1 for x in ptb_iterator(data, config.batch_size, config.num_steps))\n",
    "    total_loss = []\n",
    "    state = self.initial_state.eval()\n",
    "    #print(state)\n",
    "    #merged = tf.summary.merge_all()\n",
    "    #sum_writer = tf.summary.FileWriter(\"/tmp/logs/\",session.graph)\n",
    "    #print(merged)\n",
    "    for step, (x, y) in enumerate(\n",
    "\n",
    "      ptb_iterator(data, config.batch_size, config.num_steps)):\n",
    "      # We need to pass in the initial state and retrieve the final state to give\n",
    "      # the RNN proper history\n",
    "      #print(\"x:\",x.shape)\n",
    "      #print(\"y:\",y.shape)\n",
    "      feed = {self.input_placeholder: x,\n",
    "              self.labels_placeholder: y,\n",
    "              self.initial_state: state,\n",
    "              self.dropout_placeholder: dp}\n",
    "      #print(feed)\n",
    "#      from IPython.core.debugger import Tracer; Tracer()() \n",
    "#      print(\"calculate_loss: \",self.calculate_loss.get_shape())\n",
    "#      print(\"final_state: \",self.final_state.get_shape())\n",
    "#      print(\"train_op\", train_op.eval())\n",
    "      loss, state, _, summary = session.run(\n",
    "          [self.calculate_loss, self.final_state, train_op, self.merged], feed_dict=feed)\n",
    "      #loss, state, _ = session.run(\n",
    "      #      [self.calculate_loss, self.final_state, train_op], feed_dict=feed)\n",
    "      total_loss.append(loss)\n",
    "      self.writer.add_summary(summary,step)\n",
    "      \n",
    "#      print(loss)\n",
    "      if verbose and step % verbose == 0:\n",
    "          sys.stdout.write('\\r{} / {} : pp = {}'.format(\n",
    "              step, total_steps, np.exp(np.mean(total_loss))))\n",
    "          sys.stdout.flush()\n",
    "\n",
    "    if verbose:\n",
    "      sys.stdout.write('\\r')\n",
    "    #_loss=np.exp(np.mean(total_loss))\n",
    "    #print(\"loss:\",_loss)\n",
    "    \n",
    "    \n",
    "    return np.exp(np.mean(total_loss))\n",
    "\n",
    "def generate_text(session, model, config, starting_text='<eos>',\n",
    "                  stop_length=100, stop_tokens=None, temp=1.0):\n",
    "  \"\"\"Generate text from the model.\n",
    "\n",
    "  Hint: Create a feed-dictionary and use sess.run() to execute the model. Note\n",
    "        that you will need to use model.initial_state as a key to feed_dict\n",
    "  Hint: Fetch model.final_state and model.predictions[-1]. (You set\n",
    "        model.final_state in add_model() and model.predictions is set in\n",
    "        __init__)\n",
    "  Hint: Store the outputs of running the model in local variables state and\n",
    "        y_pred (used in the pre-implemented parts of this function.)\n",
    "\n",
    "  Args:\n",
    "    session: tf.Session() object\n",
    "    model: Object of type RNNLM_Model\n",
    "    config: A Config() object\n",
    "    starting_text: Initial text passed to model.\n",
    "  Returns:\n",
    "    output: List of word idxs\n",
    "  \"\"\"\n",
    "  state = model.initial_state.eval()\n",
    "  # Imagine tokens as a batch size of one, length of len(tokens[0])\n",
    "  tokens = [model.vocab.encode(word) for word in starting_text.split()]\n",
    "  for i in range(len(tokens) -1):\n",
    "    #print(tokens[i])\n",
    "    feed = {model.initial_state:state, model.input_placeholder:[[tokens[i]]], model.dropout_placeholder:config.dropout}\n",
    "    state, _ = session.run([model.final_state,model.predictions],feed)\n",
    "    \n",
    "  for i in range(stop_length):\n",
    "    ### YOUR CODE HERE\n",
    "    feed={model.initial_state:state, model.input_placeholder:[[tokens[-1]]], model.dropout_placeholder:config.dropout}\n",
    "    state, pred = session.run([model.final_state,model.predictions],feed)\n",
    "    ### END YOUR CODE\n",
    "    #print(\"pred:\", pred)\n",
    "    #print(\"pred 0:\", pred[0])\n",
    "    #print(\"pred flatten:\", pred.flatten())\n",
    "    next_word_idx = sample(pred[0].flatten(), temperature=temp)\n",
    "    tokens.append(next_word_idx)\n",
    "    if stop_tokens and model.vocab.decode(tokens[-1]) in stop_tokens:\n",
    "      break\n",
    "  output = [model.vocab.decode(word_idx) for word_idx in tokens]\n",
    "  return output\n",
    "\n",
    "def generate_sentence(session, model, config, *args, **kwargs):\n",
    "  \"\"\"Convenice to generate a sentence from the model.\"\"\"\n",
    "  return generate_text(session, model, config, *args, stop_tokens=['<eos>'], **kwargs)\n",
    "\n",
    "def test_RNNLM():\n",
    "  config = Config()\n",
    "  gen_config = deepcopy(config)\n",
    "  gen_config.batch_size = gen_config.num_steps = 1\n",
    "\n",
    "  # We create the training model and generative model\n",
    "  with tf.variable_scope('RNNLM') as scope:\n",
    "    model = RNNLM_Model(config)\n",
    "    # This instructs gen_model to reuse the same variables as the model above\n",
    "    scope.reuse_variables()\n",
    "    gen_model = RNNLM_Model(gen_config)\n",
    "\n",
    "  #init = tf.initialize_all_variables()\n",
    "  init = tf.global_variables_initializer()\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  with tf.Session() as session:\n",
    "\n",
    "    best_val_pp = float('inf')\n",
    "    best_val_epoch = 0\n",
    "  \n",
    "    session.run(init)\n",
    "\n",
    "    for epoch in range(config.max_epochs):\n",
    "      print('Epoch {}'.format(epoch))\n",
    "      start = time.time()\n",
    "      ###\n",
    "      #print(\"blah 1:\",model.train_step)\n",
    "      with tf.name_scope(\"epoch_train\"):\n",
    "          model.writer = tf.summary.FileWriter(\"/tmp/logs/epoch_traning/\" + str(epoch),session.graph)\n",
    "          train_pp = model.run_epoch(\n",
    "              session, model.encoded_train,\n",
    "              train_op=model.train_step\n",
    "              )\n",
    "              #)\n",
    "          model.writer.close()\n",
    "      with tf.name_scope(\"epoch_validation\"):\n",
    "          #valid_pp = model.run_epoch(session, model.encoded_valid, epoch_name=\"epoch-validation-\"+ str(epoch))\n",
    "          model.writer = tf.summary.FileWriter(\"/tmp/logs/epoch_validation/\" + str(epoch))\n",
    "          valid_pp = model.run_epoch(session, model.encoded_valid)\n",
    "          model.writer.close()\n",
    "      print('Training perplexity: {}'.format(train_pp))\n",
    "      print('Validation perplexity: {}'.format(valid_pp))\n",
    "\n",
    "      if valid_pp < best_val_pp:\n",
    "        best_val_pp = valid_pp\n",
    "        best_val_epoch = epoch\n",
    "        saver.save(session, './ptb_rnnlm.weights')\n",
    "      if epoch - best_val_epoch > config.early_stopping:\n",
    "        break\n",
    "      print('Total time: {}'.format(time.time() - start))\n",
    "\n",
    "    saver.restore(session, './ptb_rnnlm.weights')\n",
    "    #test_pp = model.run_epoch(session, model.encoded_test,epoch_name=\"epoch-test\")\n",
    "    with tf.name_scope(\"epoch_test\"):\n",
    "        model.writer = tf.summary.FileWriter(\"/tmp/logs/epoch_test/\")\n",
    "        test_pp = model.run_epoch(session, model.encoded_test)\n",
    "        model.writer.close()\n",
    "    print('=-=' * 5)\n",
    "    print('Test perplexity: {}'.format(test_pp))\n",
    "    print('=-=' * 5)\n",
    "    starting_text = 'in palo alto'\n",
    "    while starting_text:\n",
    "      print(' '.join(generate_sentence(\n",
    "          session, gen_model, gen_config, starting_text=starting_text, temp=1.0)))\n",
    "      starting_text = input('> ')\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test_RNNLM() \n",
    "    #test_text()\n",
    "    #print(\"config:\",self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
