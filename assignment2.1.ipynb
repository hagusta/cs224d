{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "q1_softmax.py codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "  Compute the softmax function in tensorflow.\n",
    "\n",
    "  You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "  tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "  not need to use all of these functions). Recall also that many common\n",
    "  tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "  if x and y are both tensors). Make sure to implement the numerical stability\n",
    "  fixes as in the previous homework!\n",
    "\n",
    "    Args:\n",
    "    x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "         represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "         input as in the previous homework)\n",
    "    Returns:\n",
    "    out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "         tensor in this problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    row=tf.shape(x)[0]\n",
    "    x_max=tf.reduce_max(x,axis=1)\n",
    "    x=tf.exp(x-tf.reshape(x_max,[row,1]))\n",
    "    x_sum=tf.reshape(tf.reduce_sum(x,axis=1),[row,1])\n",
    "    out=x/x_sum\n",
    "    ### END YOUR CODE\n",
    "      \n",
    "    return out \n",
    "\n",
    "def cross_entropy_loss(y, yhat):\n",
    "  \"\"\"\n",
    "  Compute the cross entropy loss in tensorflow.\n",
    "\n",
    "  y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "  of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "  be of dtype tf.float32.\n",
    "\n",
    "  The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "  solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "  Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "        functions.\n",
    "\n",
    "  Args:\n",
    "    y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "    yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "          probability distribution and should sum to 1.\n",
    "  Returns:\n",
    "    out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "          tensor in the problem.\n",
    "  \"\"\"\n",
    "  ### YOUR CODE HERE\n",
    "  out=-tf.reduce_sum(tf.matmul(tf.to_float(y),tf.log(yhat), transpose_b=True),axis=1)\n",
    "  ### END YOUR CODE\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "softmax tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  test1 = softmax(tf.convert_to_tensor(\n",
    "      np.array([[1001,1002],[3,4]]), dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "      test1 = test1.eval()\n",
    "  assert np.amax(np.fabs(test1 - np.array(\n",
    "      [0.26894142,  0.73105858]))) <= 1e-6\n",
    "  np.amax(np.fabs(test1 - np.array(\n",
    "      [0.26894142,  0.73105858]))) <= 1e-6\n",
    "    \n",
    "  test2 = softmax(tf.convert_to_tensor(\n",
    "      np.array([[-1001,-1002]]), dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "      test2 = test2.eval()\n",
    "  assert np.amax(np.fabs(test2 - np.array(\n",
    "      [0.73105858, 0.26894142]))) <= 1e-6\n",
    "  np.amax(np.fabs(test1 - np.array(\n",
    "      [0.26894142,  0.73105858]))) <= 1e-6\n",
    "  print(\"Basic (non-exhaustive) softmax tests pass\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_entropy_loss test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic (non-exhaustive) cross-entropy tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "  yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "  test1 = cross_entropy_loss(\n",
    "      tf.convert_to_tensor(y, dtype=tf.int32),\n",
    "      tf.convert_to_tensor(yhat, dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "    test1 = test1.eval()\n",
    "  result = -3 * np.log(.5)\n",
    "  assert np.amax(np.fabs(test1 - result)) <= 1e-6\n",
    "  print(\"Basic (non-exhaustive) cross-entropy tests pass\\n\")\n",
    "  #print(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.py codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "  \"\"\"Abstracts a Tensorflow graph for a learning task.\n",
    "\n",
    "  We use various Model classes as usual abstractions to encapsulate tensorflow\n",
    "  computational graphs. Each algorithm you will construct in this homework will\n",
    "  inherit from a Model object.\n",
    "  \"\"\"\n",
    "\n",
    "  def load_data(self):\n",
    "    \"\"\"Loads data from disk and stores it in memory.\n",
    "\n",
    "    Feel free to add instance variables to Model object that store loaded data.    \n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "  def add_placeholders(self):\n",
    "    \"\"\"Adds placeholder variables to tensorflow computational graph.\n",
    "\n",
    "    Tensorflow uses placeholder variables to represent locations in a\n",
    "    computational graph where data is inserted.  These placeholders are used as\n",
    "    inputs by the rest of the model building code and will be fed data during\n",
    "    training.\n",
    "\n",
    "    See for more information:\n",
    "\n",
    "    https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#placeholders\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "  def create_feed_dict(self, input_batch, label_batch):\n",
    "    \"\"\"Creates the feed_dict for training the given step.\n",
    "\n",
    "    A feed_dict takes the form of:\n",
    "\n",
    "    feed_dict = {\n",
    "        <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "        ....\n",
    "    }\n",
    "  \n",
    "    If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "    Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "          tensors created in add_placeholders.\n",
    "    \n",
    "    Args:\n",
    "      input_batch: A batch of input data.\n",
    "      label_batch: A batch of label data.\n",
    "    Returns:\n",
    "      feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "  def add_model(self, input_data):\n",
    "    \"\"\"Implements core of model that transforms input_data into predictions.\n",
    "\n",
    "    The core transformation for this model which transforms a batch of input\n",
    "    data into a batch of predictions.\n",
    "\n",
    "    Args:\n",
    "      input_data: A tensor of shape (batch_size, n_features).\n",
    "    Returns:\n",
    "      out: A tensor of shape (batch_size, n_classes)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "  def add_loss_op(self, pred):\n",
    "    \"\"\"Adds ops for loss to the computational graph.\n",
    "\n",
    "    Args:\n",
    "      pred: A tensor of shape (batch_size, n_classes)\n",
    "    Returns:\n",
    "      loss: A 0-d tensor (scalar) output\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "  def run_epoch(self, sess, input_data, input_labels):\n",
    "    \"\"\"Runs an epoch of training.\n",
    "\n",
    "    Trains the model for one-epoch.\n",
    "  \n",
    "    Args:\n",
    "      sess: tf.Session() object\n",
    "      input_data: np.ndarray of shape (n_samples, n_features)\n",
    "      input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "    Returns:\n",
    "      average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "  def fit(self, sess, input_data, input_labels):\n",
    "    \"\"\"Fit model on provided data.\n",
    "\n",
    "    Args:\n",
    "      sess: tf.Session()\n",
    "      input_data: np.ndarray of shape (n_samples, n_features)\n",
    "      input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "    Returns:\n",
    "      losses: list of loss per epoch\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "  def predict(self, sess, input_data, input_labels=None):\n",
    "    \"\"\"Make predictions from the provided model.\n",
    "    Args:\n",
    "      sess: tf.Session()\n",
    "      input_data: np.ndarray of shape (n_samples, n_features)\n",
    "      input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "    Returns:\n",
    "      average_loss: Average loss of model.\n",
    "      predictions: Predictions of model on input_data\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "class LanguageModel(Model):\n",
    "  \"\"\"Abstracts a Tensorflow graph for learning language models.\n",
    "\n",
    "  Adds ability to do embedding.\n",
    "  \"\"\"\n",
    "\n",
    "  def add_embedding(self):\n",
    "    \"\"\"Add embedding layer. that maps from vocabulary to vectors.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Each Model must re-implement this method.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'\n",
    "\n",
    "q1_clasifier.py code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from q1_softmax import softmax\n",
    "#from q1_softmax import cross_entropy_loss\n",
    "from model import Model\n",
    "from utils import data_iterator\n",
    "\n",
    "class Config(object):\n",
    "  \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "  The config class is used to store various hyperparameters and dataset\n",
    "  information parameters. Model objects are passed a Config() object at\n",
    "  instantiation.\n",
    "  \"\"\"\n",
    "  batch_size = 64\n",
    "  n_samples = 1024\n",
    "  n_features = 100\n",
    "  n_classes = 5\n",
    "  # You may adjust the max_epochs to ensure convergence.\n",
    "  max_epochs = 100\n",
    "  # You may adjust this learning rate to ensure convergence.\n",
    "  lr = 1e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 591.00 (0.051 sec)\n",
      "Epoch 1: loss = 0.34 (0.023 sec)\n",
      "Epoch 2: loss = 0.34 (0.027 sec)\n",
      "Epoch 3: loss = 0.33 (0.027 sec)\n",
      "Epoch 4: loss = 0.33 (0.023 sec)\n",
      "Epoch 5: loss = 0.32 (0.023 sec)\n",
      "Epoch 6: loss = 0.32 (0.021 sec)\n",
      "Epoch 7: loss = 0.31 (0.020 sec)\n",
      "Epoch 8: loss = 0.31 (0.021 sec)\n",
      "Epoch 9: loss = 0.30 (0.022 sec)\n",
      "Epoch 10: loss = 0.30 (0.024 sec)\n",
      "Epoch 11: loss = 0.30 (0.029 sec)\n",
      "Epoch 12: loss = 0.29 (0.024 sec)\n",
      "Epoch 13: loss = 0.29 (0.022 sec)\n",
      "Epoch 14: loss = 0.28 (0.021 sec)\n",
      "Epoch 15: loss = 0.28 (0.021 sec)\n",
      "Epoch 16: loss = 0.28 (0.020 sec)\n",
      "Epoch 17: loss = 0.27 (0.020 sec)\n",
      "Epoch 18: loss = 0.27 (0.021 sec)\n",
      "Epoch 19: loss = 0.27 (0.023 sec)\n",
      "Epoch 20: loss = 0.26 (0.030 sec)\n",
      "Epoch 21: loss = 0.26 (0.023 sec)\n",
      "Epoch 22: loss = 0.26 (0.023 sec)\n",
      "Epoch 23: loss = 0.25 (0.020 sec)\n",
      "Epoch 24: loss = 0.25 (0.020 sec)\n",
      "Epoch 25: loss = 0.25 (0.029 sec)\n",
      "Epoch 26: loss = 0.25 (0.032 sec)\n",
      "Epoch 27: loss = 0.24 (0.023 sec)\n",
      "Epoch 28: loss = 0.24 (0.026 sec)\n",
      "Epoch 29: loss = 0.24 (0.029 sec)\n",
      "Epoch 30: loss = 0.23 (0.024 sec)\n",
      "Epoch 31: loss = 0.23 (0.022 sec)\n",
      "Epoch 32: loss = 0.23 (0.021 sec)\n",
      "Epoch 33: loss = 0.23 (0.020 sec)\n",
      "Epoch 34: loss = 0.22 (0.021 sec)\n",
      "Epoch 35: loss = 0.22 (0.020 sec)\n",
      "Epoch 36: loss = 0.22 (0.020 sec)\n",
      "Epoch 37: loss = 0.22 (0.020 sec)\n",
      "Epoch 38: loss = 0.22 (0.024 sec)\n",
      "Epoch 39: loss = 0.21 (0.029 sec)\n",
      "Epoch 40: loss = 0.21 (0.024 sec)\n",
      "Epoch 41: loss = 0.21 (0.023 sec)\n",
      "Epoch 42: loss = 0.21 (0.022 sec)\n",
      "Epoch 43: loss = 0.21 (0.020 sec)\n",
      "Epoch 44: loss = 0.20 (0.020 sec)\n",
      "Epoch 45: loss = 0.20 (0.021 sec)\n",
      "Epoch 46: loss = 0.20 (0.021 sec)\n",
      "Epoch 47: loss = 0.20 (0.020 sec)\n",
      "Epoch 48: loss = 0.20 (0.025 sec)\n",
      "Epoch 49: loss = 0.19 (0.026 sec)\n",
      "Epoch 50: loss = 0.19 (0.027 sec)\n",
      "Epoch 51: loss = 0.19 (0.026 sec)\n",
      "Epoch 52: loss = 0.19 (0.026 sec)\n",
      "Epoch 53: loss = 0.19 (0.021 sec)\n",
      "Epoch 54: loss = 0.19 (0.021 sec)\n",
      "Epoch 55: loss = 0.18 (0.021 sec)\n",
      "Epoch 56: loss = 0.18 (0.022 sec)\n",
      "Epoch 57: loss = 0.18 (0.023 sec)\n",
      "Epoch 58: loss = 0.18 (0.028 sec)\n",
      "Epoch 59: loss = 0.18 (0.027 sec)\n",
      "Epoch 60: loss = 0.18 (0.032 sec)\n",
      "Epoch 61: loss = 0.18 (0.028 sec)\n",
      "Epoch 62: loss = 0.17 (0.024 sec)\n",
      "Epoch 63: loss = 0.17 (0.021 sec)\n",
      "Epoch 64: loss = 0.17 (0.020 sec)\n",
      "Epoch 65: loss = 0.17 (0.021 sec)\n",
      "Epoch 66: loss = 0.17 (0.027 sec)\n",
      "Epoch 67: loss = 0.17 (0.027 sec)\n",
      "Epoch 68: loss = 0.17 (0.023 sec)\n",
      "Epoch 69: loss = 0.17 (0.025 sec)\n",
      "Epoch 70: loss = 0.16 (0.028 sec)\n",
      "Epoch 71: loss = 0.16 (0.029 sec)\n",
      "Epoch 72: loss = 0.16 (0.023 sec)\n",
      "Epoch 73: loss = 0.16 (0.022 sec)\n",
      "Epoch 74: loss = 0.16 (0.022 sec)\n",
      "Epoch 75: loss = 0.16 (0.025 sec)\n",
      "Epoch 76: loss = 0.16 (0.027 sec)\n",
      "Epoch 77: loss = 0.16 (0.025 sec)\n",
      "Epoch 78: loss = 0.15 (0.024 sec)\n",
      "Epoch 79: loss = 0.15 (0.022 sec)\n",
      "Epoch 80: loss = 0.15 (0.022 sec)\n",
      "Epoch 81: loss = 0.15 (0.022 sec)\n",
      "Epoch 82: loss = 0.15 (0.021 sec)\n",
      "Epoch 83: loss = 0.15 (0.021 sec)\n",
      "Epoch 84: loss = 0.15 (0.031 sec)\n",
      "Epoch 85: loss = 0.15 (0.030 sec)\n",
      "Epoch 86: loss = 0.15 (0.031 sec)\n",
      "Epoch 87: loss = 0.15 (0.031 sec)\n",
      "Epoch 88: loss = 0.14 (0.022 sec)\n",
      "Epoch 89: loss = 0.14 (0.021 sec)\n",
      "Epoch 90: loss = 0.14 (0.021 sec)\n",
      "Epoch 91: loss = 0.14 (0.031 sec)\n",
      "Epoch 92: loss = 0.14 (0.026 sec)\n",
      "Epoch 93: loss = 0.14 (0.030 sec)\n",
      "Epoch 94: loss = 0.14 (0.027 sec)\n",
      "Epoch 95: loss = 0.14 (0.024 sec)\n",
      "Epoch 96: loss = 0.14 (0.023 sec)\n",
      "Epoch 97: loss = 0.14 (0.025 sec)\n",
      "Epoch 98: loss = 0.14 (0.030 sec)\n",
      "Epoch 99: loss = 0.13 (0.023 sec)\n",
      "Basic (non-exhaustive) classifier tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SoftmaxModel(Model):\n",
    "  \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "  def load_data(self):\n",
    "    \"\"\"Creates a synthetic dataset and stores it in memory.\"\"\"\n",
    "    np.random.seed(1234)\n",
    "    self.input_data = np.random.rand(\n",
    "        self.config.n_samples, self.config.n_features)\n",
    "    self.input_labels = np.ones((self.config.n_samples,), dtype=np.int32)\n",
    "\n",
    "  def add_placeholders(self):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "    These placeholders are used as inputs by the rest of the model building\n",
    "    code and will be fed data during training.\n",
    "\n",
    "    Adds following nodes to the computational graph\n",
    "\n",
    "    input_placeholder: Input placeholder tensor of shape\n",
    "                       (batch_size, n_features), type tf.float32\n",
    "    labels_placeholder: Labels placeholder tensor of shape\n",
    "                       (batch_size, n_classes), type tf.int32\n",
    "\n",
    "    Add these placeholders to self as the instance variables\n",
    "  \n",
    "      self.input_placeholder\n",
    "      self.labels_placeholder\n",
    "\n",
    "    (Don't change the variable names)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #cfg=Config()\n",
    "    self.input_placeholder=tf.placeholder(tf.float32,[self.config.batch_size,self.config.n_features],name=\"input_placeholder\")\n",
    "    self.labels_placeholder=tf.placeholder(tf.int32,[self.config.batch_size,self.config.n_classes],name=\"labels_placeholder\")\n",
    "    ### END YOUR CODE\n",
    "\n",
    "  def create_feed_dict(self, input_batch, label_batch):\n",
    "    \"\"\"Creates the feed_dict for softmax classifier.\n",
    "\n",
    "    A feed_dict takes the form of:\n",
    "\n",
    "    feed_dict = {\n",
    "        <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "        ....\n",
    "    }\n",
    "\n",
    "    If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "    Hint: The keys for the feed_dict should match the placeholder tensors\n",
    "          created in add_placeholders.\n",
    "    \n",
    "    Args:\n",
    "      input_batch: A batch of input data.\n",
    "      label_batch: A batch of label data.\n",
    "    Returns:\n",
    "      feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    feed_dict= {self.input_placeholder:input_batch, self.labels_placeholder:label_batch}\n",
    "    ### END YOUR CODE\n",
    "    return feed_dict\n",
    "\n",
    "  def add_training_op(self, loss):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train. See \n",
    "\n",
    "    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "    for more information.\n",
    "\n",
    "    Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "          Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "    Args:\n",
    "      loss: Loss tensor, from cross_entropy_loss.\n",
    "    Returns:\n",
    "      train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    #print(\"add_training_op NotImplemented\")\n",
    "    optimizer=tf.train.GradientDescentOptimizer(self.config.lr)\n",
    "    train_op=optimizer.minimize(loss)\n",
    "    ### END YOUR CODE\n",
    "    return train_op\n",
    "\n",
    "  def add_model(self, input_data):\n",
    "    \"\"\"Adds a linear-layer plus a softmax transformation\n",
    "\n",
    "    The core transformation for this model which transforms a batch of input\n",
    "    data into a batch of predictions. In this case, the mathematical\n",
    "    transformation effected is\n",
    "\n",
    "    y = softmax(xW + b)\n",
    "\n",
    "    Hint: Make sure to create tf.Variables as needed. Also, make sure to use\n",
    "          tf.name_scope to ensure that your name spaces are clean.\n",
    "    Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "          and biases b with zeros.\n",
    "\n",
    "    Args:\n",
    "      input_data: A tensor of shape (batch_size, n_features).\n",
    "    Returns:\n",
    "      out: A tensor of shape (batch_size, n_classes)\n",
    "    \"\"\"\n",
    "    tf.name_scope(\"Softmax\")\n",
    "    ### YOUR CODE HERE\n",
    "    #if hasattr(self, 'weights') and self.weights is not none:\n",
    "    #    weights = self.weights\n",
    "    #else:\n",
    "    weights=tf.Variable(tf.truncated_normal([self.config.n_classes,self.config.n_features],\n",
    "                                            stddev=1.0 / math.sqrt(self.config.n_features)))\n",
    "    #if hasattr(self, 'biases') and self.weights is not none:\n",
    "    #    biases = self.biases\n",
    "    #else:\n",
    "    biases=tf.Variable(tf.zeros(shape=[self.config.n_classes]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    #h=tf.Variable(tf.zeros(shape=[self.config.batch_size,self.config.n_classes]))\n",
    "    \n",
    "    #print(\"add_model NotImplemented\")\n",
    "    #y=softmax(tf.matmul(weights,input_data,transpose_b=True)+biases)\n",
    "    #h=tf.matmul(weights,input_data,transpose_b=True)\n",
    "    h=tf.matmul(input_data,weights,transpose_b=True) + biases\n",
    "    h=softmax(h)\n",
    "    out=h\n",
    "    ### END YOUR CODE\n",
    "    return out\n",
    "\n",
    "  def add_loss_op(self, pred):\n",
    "    \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "    Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "          short function.\n",
    "    Args:\n",
    "      pred: A tensor of shape (batch_size, n_classes)\n",
    "    Returns:\n",
    "      loss: A 0-d tensor (scalar)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #print(\"add_loss_op NotImplemented\")\n",
    "    loss=tf.reduce_sum(cross_entropy_loss(self.labels_placeholder,pred))\n",
    "    ### END YOUR CODE\n",
    "    return loss\n",
    "\n",
    "  def run_epoch(self, sess, input_data, input_labels):\n",
    "    \"\"\"Runs an epoch of training.\n",
    "\n",
    "    Trains the model for one-epoch.\n",
    "  \n",
    "    Args:\n",
    "      sess: tf.Session() object\n",
    "      input_data: np.ndarray of shape (n_samples, n_features)\n",
    "      input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "    Returns:\n",
    "      average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "    \"\"\"\n",
    "    # And then after everything is built, start the training loop.\n",
    "    average_loss = 0\n",
    "    for step, (input_batch, label_batch) in enumerate(\n",
    "        data_iterator(input_data, input_labels,\n",
    "                      batch_size=self.config.batch_size,\n",
    "                      label_size=self.config.n_classes)):\n",
    "\n",
    "      # Fill a feed dictionary with the actual set of images and labels\n",
    "      # for this particular training step.\n",
    "      feed_dict = self.create_feed_dict(input_batch, label_batch)\n",
    "\n",
    "      # Run one step of the model.  The return values are the activations\n",
    "      # from the `self.train_op` (which is discarded) and the `loss` Op.  To\n",
    "      # inspect the values of your Ops or variables, you may include them\n",
    "      # in the list passed to sess.run() and the value tensors will be\n",
    "      # returned in the tuple from the call.\n",
    "      _, loss_value = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "      average_loss += loss_value\n",
    "\n",
    "    average_loss = average_loss / step\n",
    "    return average_loss \n",
    "\n",
    "  def fit(self, sess, input_data, input_labels):\n",
    "    \"\"\"Fit model on provided data.\n",
    "\n",
    "    Args:\n",
    "      sess: tf.Session()\n",
    "      input_data: np.ndarray of shape (n_samples, n_features)\n",
    "      input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "    Returns:\n",
    "      losses: list of loss per epoch\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for epoch in range(self.config.max_epochs):\n",
    "      start_time = time.time()\n",
    "      average_loss = self.run_epoch(sess, input_data, input_labels)\n",
    "      duration = time.time() - start_time\n",
    "      # Print status to stdout.\n",
    "      print(('Epoch %d: loss = %.2f (%.3f sec)'\n",
    "             % (epoch, average_loss, duration)))\n",
    "      losses.append(average_loss)\n",
    "    return losses\n",
    "\n",
    "  def __init__(self, config):\n",
    "    \"\"\"Initializes the model.\n",
    "\n",
    "    Args:\n",
    "      config: A model configuration object of type Config\n",
    "    \"\"\"\n",
    "    self.config = config\n",
    "    # Generate placeholders for the images and labels.\n",
    "    self.load_data()\n",
    "    self.add_placeholders()\n",
    "    self.pred = self.add_model(self.input_placeholder)\n",
    "    self.loss = self.add_loss_op(self.pred)\n",
    "    self.train_op = self.add_training_op(self.loss)\n",
    "  \n",
    "def test_SoftmaxModel():\n",
    "  \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "  config = Config()\n",
    "  with tf.Graph().as_default():\n",
    "    model = SoftmaxModel(config)\n",
    "  \n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "  \n",
    "    # Run the Op to initialize the variables.\n",
    "    #init = tf.initialize_all_variables()()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "  \n",
    "    losses = model.fit(sess, model.input_data, model.input_labels)\n",
    "\n",
    "  # If ops are implemented correctly, the average loss should fall close to zero\n",
    "  # rapidly.\n",
    "  assert losses[-1] < .5\n",
    "  print(\"Basic (non-exhaustive) classifier tests pass\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_SoftmaxModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
